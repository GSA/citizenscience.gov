---
id: 21202
title: 'The Aggregative Contingent Estimation Program: Predicting Global Events Through Crowdsourcing'
date: 2015-08-25T18:14:42+00:00
author: Kendrick Daniel
layout: posts
guid: https://crowdsourcing-toolkit.sites.usa.gov/?p=21202
permalink: /2015/08/25/ace-forecasting/
hide_title:
  - 'No'
header_layout:
  - Default
centercatnav:
  - 'No'
remove_thumb:
  - 'No'
hide_auth_bio:
  - 'No'
post_featcontent:
  - 'No'
post_featpages:
  - 'No'
amazonS3_cache:
  - 'a:4:{s:93:"https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1054/2015/08/case-study-ace-01.gif";i:21582;s:95:"https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1054/2015/08/case-study-ace-main.gif";i:21592;s:103:"https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1054/2015/08/case-study-ace-main-300x174.gif";i:21592;s:91:"https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1054/2015/09/ace-forecasting.pdf";i:24902;}'
image: https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1054/2015/09/feature-ace.gif
categories:
  - All
  - Research
tags:
  - College or University
  - Computer-based
  - data analysis
  - Decision-making
  - for-profit
  - General Public
  - Intelligence Advanced Research Projects Activity
  - International
  - Organization awareness
  - social sciences
---
## Case Study Overview

<div id="attachment_21582" style="width: 410px" class="wp-caption alignright">
  <img class="wp-image-21582 size-full" src="https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1054/2015/08/case-study-ace-01.gif" alt="graph showing probabilities over time for different events, and the number of inputs for each time period." width="400" height="203" />
  
  <p class="wp-caption-text">
    Notional <em>ACE</em> system output.
  </p>
</div>

The goal of the <span style="font-style: normal !msorm"><em>Aggregative Contingent Estimation</em></span> Program, sponsored by the Intelligence Advanced Research Projects Activity, is to enhance the accuracy, precision and timeliness of forecasts for a broad range of global events. The program develops advanced techniques to gather, weight and combine the judgments of people from many backgrounds and fields and in many different locations.

_ACE_ is powered by human judgment, which makes it flexible enough to provide forecasts on just about any type of intelligence-forecasting question. Launched in 2010, _ACE_ is based on the idea that combining forecasts made by an informed and diverse group of people often produces more accurate predictions of future events than those made by a single expert.

<a href="https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1054/2015/09/ace-forecasting.pdf" target="_blank">Download this case study (PDF, 54KB)<br /> </a>Website: _[ACE](http://www.iarpa.gov/index.php/research-programs/ace)_

## Project Description

_ACE_ started with a &#8220;forecasting tournament.&#8221; Five teams of leading scholars from industry and academia competed to forecast events. They recruited thousands of research participants; each year, the participants answered about 100 questions related to social, economic and political events. Every day, the teams sent forecasts to an independent evaluator, who scored them based on actual outcomes. Each research team tried to produce the most accurate forecasts, competing against each other and against a benchmark group that used the unweighted average judgment of a group of forecasters.

After two years, one research team — the Good Judgment Project&#8211;substantially outperformed the others. In fact, Good Judgment’s improvement in accuracy was greater than the improvement of the other four research teams combined — about 70 percent over the benchmark. Forecast improvement was measured using Brier scoring, a method originally developed to evaluate weather forecasts.

## Challenges

<div id="attachment_21592" style="width: 310px" class="wp-caption alignright">
  <img class="wp-image-21592 size-medium" src="https://s3.amazonaws.com/sitesusa/wp-content/uploads/sites/1054/2015/08/case-study-ace-main-300x174.gif" alt="Weighting algorithm with icons of person sitting at a desk in front of a computer and the equation at the bottom." width="300" height="174" />
  
  <p class="wp-caption-text">
    Schematic drawing of the notional ACE weighting algorithm.
  </p>
</div>

Four of the five teams had difficulty recruiting and retaining the number of people they needed, because continuous forecasting was somewhat time-consuming (taking about one hour per week). Teams also had to decide how best to use project resources —  and whether to focus most of their effort on finding the best ways to efficiently collect probability judgments; on determining how best to combine and weight those judgments; or on developing training methods for forecasters.

  * Collecting judgments involved finding the best way to gather the needed range and number of probabilistic beliefs from a crowd of individuals — whether by surveys, by prediction markets, or by some other technique — and then producing the most intuitive and user-friendly interfaces for these platforms.
  * Combining judgments involved developing new algorithms to create the most accurate aggregated forecasts.
  * Training involved teaching forecasters the skills that would help them become more accurate and less susceptible to judgmental biases or poor decisionmaking.

In addition, the project’s initial concept faced resistance from potential participants and customers. Analysts are not often trained to think in quantitative terms and may be reluctant to provide numerical forecasts that can be scored for accuracy. However, letting forecasters be anonymous made it easier for them to take the risk and to take the time to develop the skills needed.

## Benefits and Outcomes

The team that won the _ACE_ tournament (the Good Judgment Project) made substantial advances in all three areas:

  * _Collecting judgments_: Given the advanced algorithms generated in _ACE_, opinion surveys surpassed prediction market platforms as the best way to elicit probabilistic judgments from forecasters.
  * _Combining judgments_: Promising new algorithms weighted individual survey responses based on past accuracy, then pushed up some probability judgments (for example, an average prediction of 70 percent might be pushed to 90 percent if the beliefs of previously accurate forecasters warranted it). This dramatically increased the accuracy of the combined judgments.
  * _Training forecasters_: The team created a one-hour online class that improved individual forecaster accuracy by about 10 percent.

_AC__E_ shows that meaningful geopolitical forecasts can be produced quickly and accurately on topics ranging from violent international confrontations to how long international leaders will stay in power. By better measuring exact levels of uncertainty, the project can also increase the rigor of intelligence analysis more generally. For the first time, we have a quantitative system flexible enough for rapid analysis of almost any subject. Where traditional analysis can take days or weeks, _ACE_ forecasts can be obtained in a matter of hours. Consumers of _ACE_ forecasts can be confident in their accuracy because the technologies have been validated in a real-world forecasting tournament.

## Tips

The _ACE_ case study illustrates the following steps in the Federal Citizen Science and Crowdsourcing Toolkit:

  * **[Design a Project](https://crowdsourcing-toolkit.sites.usa.gov/step-2-design-a-project/) —**** Know Your Objectives**_._  The objective in this case was to improve forecasting by more than 50 percent over the state-of-the-art forecasts. Choosing a clear, measurable target and having a state-of-the-art control group as a benchmark enabled progress to be clearly gauged. By the program’s end, the Good Judgment Project beat the state-of-the-art forecasts by more than 70 percent. Setting specific quantitative performance benchmarks is a hallmark of all IARPA programs, and _ACE_ was no different.

  * **[Build a Community](https://crowdsourcing-toolkit.sites.usa.gov/step-3-engage-your-community/) —**** Engage Your Community.** The Good Judgment Project recruited and retained an impressive pool of high-quality participants. Participants were highly credentialed (some 60 percent had graduate degrees) and tenacious. Many spent dozens of hours per week forecasting, even though they were paid only a couple hundred dollars per year in Amazon gift cards. The Good Judgment Project understood its pool of participants, providing ongoing feedback on individual accuracy to encourage participation and ongoing effort. The project recognized and rewarded exceptional forecasters as “superforecasters.” The result was a uniquely engaged and loyal group of participants.

&nbsp;

[wpex more=&#8221;Learn More&#8221; less=&#8221;Read less&#8221;]

  * Website: _[ACE](http://www.iarpa.gov/index.php/research-programs/ace)_
  * [Principles of Community Engagement, NIH (2011)](http://www.atsdr.cdc.gov/communityengagement/pdf/PCE_Report_508_FINAL.pdf)
  * [Using Crowdsourcing in Government, IBM Center for The Business of Government (2013)](http://www.businessofgovernment.org/sites/default/files/Using%20Crowdsourcing%20In%20Government.pdf)
  * [Community Engagement Techniques: Best Practices (2014)](https://www.herefordshire.gov.uk/media/6312587/12_best_practice_community_engagement_techniques.pdf)
  * [Crowdsourcing: A Geographic Approach to Public Engagement, SSRN (2014)](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2518233)

[/wpex]

&nbsp;

## Contact Information

Dr. Steve Rieber
  
Email: <steven.rieber@iarpa.gov>
  
</br>

Last updated: <span class="last-modified-timestamp">May 31, 2016</span>